### 7.1 벡터 공간 모델

+ 벡터 공간 모델
  + 텍스트 문서를 색인어와 같은 식별자들을 **벡터**로 나타내는 대수적인 모델
  + 문서는 벡터로 표현되며, 각각의 차원은 개별 단어에 대응됨
  + 만약 문서 내에 특정 단어가 포함되어 있다면. 벡터 내에서 해당 차원은 0이 아닌 값을 갖게 됨
+ 단어 가중치 (Term Weight)
  + 여러가지 계산 방법이 있으며, 가장 잘 알려진 방법 중의 하나는 TF-IDF 가중치를 구하는 방법
  + 단어는 하나의 단어이거나 키워드, 혹은 더 긴 구가 될 수 있음
  + 만약 단어가 단일어로 선택된다면, 벡터의 차원 수 (Dimensionality)는 단어집(Vocabulary) 내의 단어들의 숫자(언어 자료 내에 들어 있는 개별 단어들의 숫자)가 됨
+ 벡터 공간 모델의 응용
  + 문서벡터(Document Vector)
  + 검색의 대상이 되는 문서 : D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>,  ...D<sub>n</sub>이고, 이와 같은 문서 집합 전체에 결쳐 전부 m개의 색인어 W<sub>1</sub>, W<sub>2</sub>, W<sub>3</sub>,  …W<sub>m</sub>
  + 문서 집합의 전체는 m * n 행렬로 표현할 수 있음
  + 문헌 * 단어  매트릭스(Document*Term Matrix)
    + 각 열은 색인어(Term Vector)
+ 벡터 공간 모델의 한계
  + 긴 문서들은 유사도 값이 작기 때문에, 제대로 표현되지 않음
  + 비슷한 의미를 갖고 있는 문서들일지라도 사용된 단어가 다르면 연관성을 갖지 못함
  + 벡터 공간 표현에서는 단어들이 나타나는 **순서가 무시되기 때문에** 문맥 정보를 파악할 수 없음

### 7-2 문헌 * 단어 매트릭스 생성

+ 문헌*단어 매트릭스 생성은 다양한 방법이 가능
  + LuceneVectorSpaceModelManager
    + 루씬을 이용해서 문헌집단을 색인한 후 문헌*단어 매트릭스를 만들 수 있음 **(먼저 색인을 해야한다는 단점이 있음)**
    + 메모리가 적게 소요되고 효율적 (**개인PC에서도 사용가능**)
  + 분산병렬방법 사용(Hadoop)
    + 루씬보다 더 복잡
    + 많은 양의 데이터 처리 가능
  + createDocumentTermMatrix 함수
    + 문헌열과 문헌집단 안에 존재하는 어휘 개수를 인수로 받음
+ 문헌집단의 수를 처리하는 데 필요한 메모리보다 코드를 실행하는 **컴퓨터의 메모리 용량이 작을 때는** 문헌*단어 매트릭스를 생성할 수 없음

### 7-3 단어 가중치 기법

+ 단어 가종치 기법
  + 단순 빈도에 의존한 단어의 중요도를 분석하는 데 단점들을 보완하고자 초기 정보검색론에서 제안되었음(Salton and Buckley, 1998)
  + 단어 가중치는 텍스트 분류처럼 정보검색과 지도학습에서 널리 사용됨

+ 기본 가정

  + 문헌에서 자주 나타나는 단어는 문헌이 무엇에 관한 것인지 기술할 것
  + 문헌의 유용한 용어 집단을 다른 용어들과 **차별적으로** 나타낼 수 있음

+ Zipf's law

  + 불용어와 같은 기능어로 용어 분포에 따라 문헌들 사이에서 우세
  + 단일어 카운팅은 별로 유용하지 않음

+ TF*IDF (Term Frequency X Inverse Document Frequency)

  + 단어 가중치 기법 중 가장 일반적으로 알려진 알고리즘
  + 특정 문서 내에서 단어 빈도가 높을수록, 전체 문서들에는 그 단어를 포함한 문서가 적을 수록 TF*IDF 값이 높아짐
  + 이 값을 이용하여 모둔 문서에 나타나는 흔한 단어들을 걸러내며, 특정 단어가 가지는 **중요도를** 측정하는데 사용됨

+ 단어 빈도 (TF)

  + 주어진 문서에 대한 단어 빈도는 단순히 그 문서에서 해당 단어가 나타나는 빈도수
  + 문서의 길이가 길면 해당 단어의 실제 중요도와는 상관없이 단어의 빈도수는 증가될 확률이 높음
  + 특정 문서 d<sub>j</sub>에서 단어 t<sub>i</sub>의 중요도 : tf<sub>i,j</sub>
    + 분자 : 문서 d<sub>j</sub>에서 단어 t<sub>i</sub>가 나타나는 빈도수 : n<sub>i,j</sub>
    + 분모 : 문서 d<sub>j</sub> 내의 모든 단어가 나타내는 빈도수 : 시그마<sub>k</sub> n<sub>k,j</sub>

+ 역문헌 빈도(IDF)

  + 해당 단어의 일반적인 중요도를 나타내는 값
  + 전체 문서의 수를 해당 단어가 포함된 문서들의 수로 나눈 값에 로그를 취해서 얻을 수 있음

+ TF*IDF  가중치는 단어 빈도와 역문헌 빈도의 곱

+ 엔트로피(Entropy)

  + 물리계의 무질서의 정도

  + 정보이론에서는 정보의 불확실성

  + 불확실성이 높을수록 엔트로피는 커짐 (Shannon, 1948)

    h(x) = -log<sub>2</sub>p(x)

  + 정보의 불확실성 값 : h(x)

  + 확률 : p(x)

  + 예시) 사건 '가, 나, 다, 라, 마'

    + 각 사건의 발생확률이 각각 1/5 라면, 전체 사건에 대한 엔트로피는 {-1/5 * log<sub>2</sub>(1/5)}*5 = 2.321928

    + 각 사건의 발생확률이 각각 4/10, 2/10, 1/10, 1/10, 2/10이라면. 전체사건에 대한 엔트로피는

      -4/10 * log<sub>2</sub>(4/10) + {-2/10*log<sub>2</sub>(2/10)} * 2 + {-1/10 *log<sub>2</sub>(1/10} * 2 = 2.121928095

    + 두번째가 엔트로피가 더 적다 = 불확실성이 더 적다

+ 상호 정보량(Mutual Information)

  + 사건 A와 사건 B의 상호 정보량

  + 사건 A가 일어날 확률과 사건 B가 일어날 확률 중 사건 A와 B가 **동시에** 일어날 확률

  + 사건 A와 사건 B의 상호 정보량

    MI(A, B) = P(A 교집합 B) / (P(A) * P(B))

    + 사건 A가 일어날 확률 P(A)
    + 사건 B가 일어날 확률 P(B)

+ 카이제곱 (Chi-Squre)

  + 통계학에서 가정 널리 사용되는 통계 검정 중 하나로, 명목척도를 이용하여 측정된 자료에서 **연관성을 찾을 때** 통상적으로 사용됨
  + 모든 검정은 카이제곱 분포에 근거하고 **실제로 관찰된 값과 결과로 기대되는 값 사이의 차이**를 찾는 것에 기초함
  + 텍스타마이닝에서는 보통 기계학습에서 **자질**을 선정하는 방법으로 사용됨
  + 텍스트 자동 분류를 할 때 문헌집단에 출현한 모든 단어 중에 중요단어를 자질로 선정하기 위해 사용되기도 함
  + 문헌집단에 출현하는 **단어들의 빈도수**와 밀접한 관계가 있음
  + 정규분포를 따르는 모집단에서 크기가 n인 표본을 무작위로 반복하여 추출하였을 때, 각 표본에 대해 구한 **표본분산**들은 카이제곱 분포를 따름
  + 카이제곱 확률변수는 **자유도 (n-1)인 카이제곱 분포**를 따름

